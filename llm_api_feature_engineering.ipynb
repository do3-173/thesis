{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e194f9ae",
   "metadata": {},
   "source": [
    "# LLM-Based Feature Selection Methods Comparison\n",
    "\n",
    "This notebook compares different LLM-based feature selection methods with traditional approach.\n",
    "\n",
    "## Methods Implemented:\n",
    "1. **Text-based Feature Selection** (Li et al. 2024)\n",
    "2. **LLM4FS Hybrid Approach** (Li & Xiu 2025) \n",
    "3. **Traditional Baselines** (Random Forest, Mutual Information)\n",
    "\n",
    "## Datasets (All Classification):\n",
    "- **CMC** (Contraceptive Method Choice) - 3 classes\n",
    "- **Vehicle** - 4 classes  \n",
    "- **Electricity** - 2 classes\n",
    "\n",
    "## Evaluation Metric:\n",
    "- **Accuracy** for all classification tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20132366",
   "metadata": {},
   "source": [
    "## Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28410cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# API clients\n",
    "import anthropic\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358230d9",
   "metadata": {},
   "source": [
    "## API Key Configuration\n",
    "\n",
    "Create a `.env` file in your thesis directory with your API keys:\n",
    "\n",
    "```\n",
    "ANTHROPIC_API_KEY=your_anthropic_key_here\n",
    "OPENAI_API_KEY=your_openai_key_here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72257315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key Status:\n",
      "Anthropic API Key: Found\n",
      "OpenAI API Key: Found\n",
      "Anthropic client initialized\n",
      "OpenAI client initialized\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Check for API keys\n",
    "anthropic_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "openai_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "print(\"API Key Status:\")\n",
    "print(f\"Anthropic API Key: {'Found' if anthropic_key else 'Not found'}\")\n",
    "print(f\"OpenAI API Key: {'Found' if openai_key else 'Not found'}\")\n",
    "\n",
    "# Initialize API clients\n",
    "anthropic_client = None\n",
    "openai_client = None\n",
    "\n",
    "if anthropic_key:\n",
    "    anthropic_client = anthropic.Anthropic(api_key=anthropic_key)\n",
    "    print(\"Anthropic client initialized\")\n",
    "\n",
    "if openai_key:\n",
    "    openai_client = openai.OpenAI(api_key=openai_key)\n",
    "    print(\"OpenAI client initialized\")\n",
    "\n",
    "if not anthropic_key and not openai_key:\n",
    "    print(\"\\nNo API keys found. Please add them to your .env file.\")\n",
    "    print(\"Create a .env file in your thesis directory with:\")\n",
    "    print(\"ANTHROPIC_API_KEY=your_key_here\")\n",
    "    print(\"OPENAI_API_KEY=your_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd9dc9",
   "metadata": {},
   "source": [
    "## Dataset Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c5ffb07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available datasets:\n",
      " 1. cmc\n",
      " 2. connect-4\n",
      " 3. electricity\n",
      " 4. eye_movements\n",
      " 5. kc1\n",
      " 6. phoneme\n",
      " 7. pol\n",
      " 8. splice\n",
      " 9. vehicle\n",
      "\n",
      "Loading example dataset: cmc\n",
      "Successfully loaded cmc\n",
      "Shape: (1473, 10)\n",
      "Columns: ['Wifes_age', 'Wifes_education', 'Husbands_education', 'Number_of_children_ever_born', 'Wifes_religion', 'Wifes_now_working%3F', 'Husbands_occupation', 'Standard-of-living_index', 'Media_exposure', 'target']\n"
     ]
    }
   ],
   "source": [
    "# Dataset paths\n",
    "DATASET_DIR = Path(\"datasets_csv\")\n",
    "\n",
    "# Available datasets from your thesis tasks\n",
    "AVAILABLE_DATASETS = [\n",
    "    'cmc', 'connect-4', 'electricity', 'eye_movements', \n",
    "    'kc1', 'phoneme', 'pol', 'splice', 'vehicle'\n",
    "]\n",
    "\n",
    "def load_dataset(dataset_name: str) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Load dataset and its metadata.\n",
    "    \n",
    "    Args:\n",
    "        dataset_name: Name of the dataset to load\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (dataframe, metadata_dict)\n",
    "    \"\"\"\n",
    "    csv_path = DATASET_DIR / f\"{dataset_name}.csv\"\n",
    "    metadata_path = DATASET_DIR / f\"{dataset_name}_metadata.json\"\n",
    "    \n",
    "    if not csv_path.exists():\n",
    "        raise FileNotFoundError(f\"Dataset {dataset_name} not found at {csv_path}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Load metadata if available\n",
    "    metadata = {}\n",
    "    if metadata_path.exists():\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "    \n",
    "    return df, metadata\n",
    "\n",
    "def get_dataset_info(dataset_name: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Get comprehensive information about a dataset.\n",
    "    \"\"\"\n",
    "    df, metadata = load_dataset(dataset_name)\n",
    "    \n",
    "    info = {\n",
    "        'name': dataset_name,\n",
    "        'shape': df.shape,\n",
    "        'columns': list(df.columns),\n",
    "        'dtypes': df.dtypes.to_dict(),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'metadata': metadata,\n",
    "        'sample_data': df.head(3).to_dict('records')\n",
    "    }\n",
    "    \n",
    "    return info\n",
    "\n",
    "# Test loading a dataset\n",
    "print(\"Available datasets:\")\n",
    "for i, dataset in enumerate(AVAILABLE_DATASETS, 1):\n",
    "    print(f\"{i:2d}. {dataset}\")\n",
    "\n",
    "# Load first dataset as example\n",
    "if AVAILABLE_DATASETS:\n",
    "    test_dataset = AVAILABLE_DATASETS[0]\n",
    "    print(f\"\\nLoading example dataset: {test_dataset}\")\n",
    "    try:\n",
    "        df_test, metadata_test = load_dataset(test_dataset)\n",
    "        print(f\"Successfully loaded {test_dataset}\")\n",
    "        print(f\"Shape: {df_test.shape}\")\n",
    "        print(f\"Columns: {list(df_test.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {test_dataset}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0a4827",
   "metadata": {},
   "source": [
    "## LLM API Interface Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b165abc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing API connections...\n",
      "\n",
      "Anthropic API connection successful\n",
      "Model: claude-3-haiku-20240307\n",
      "Response: Connection successful...\n",
      "\n",
      "Openai API connection successful\n",
      "Model: gpt-3.5-turbo\n",
      "Response: Connection successful...\n"
     ]
    }
   ],
   "source": [
    "class LLMInterface:\n",
    "    \"\"\"\n",
    "    Base interface for LLM API calls.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, provider: str = \"anthropic\", model: str = None):\n",
    "        self.provider = provider\n",
    "        self.model = model or self._get_default_model()\n",
    "        self.client = self._initialize_client()\n",
    "        \n",
    "    def _get_default_model(self) -> str:\n",
    "        \"\"\"Get default model for each provider.\"\"\"\n",
    "        defaults = {\n",
    "            \"anthropic\": \"claude-3-haiku-20240307\",\n",
    "            \"openai\": \"gpt-3.5-turbo\"\n",
    "        }\n",
    "        return defaults.get(self.provider, \"claude-3-haiku-20240307\")\n",
    "    \n",
    "    def _initialize_client(self):\n",
    "        \"\"\"Initialize the appropriate API client.\"\"\"\n",
    "        if self.provider == \"anthropic\" and anthropic_client:\n",
    "            return anthropic_client\n",
    "        elif self.provider == \"openai\" and openai_client:\n",
    "            return openai_client\n",
    "        else:\n",
    "            raise ValueError(f\"No API client available for provider: {self.provider}\")\n",
    "    \n",
    "    def call_llm(self, prompt: str, max_tokens: int = 1000, temperature: float = 0.1) -> str:\n",
    "        \"\"\"\n",
    "        Make API call to LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to send to the LLM\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            temperature: Temperature for generation (0.0 to 1.0)\n",
    "            \n",
    "        Returns:\n",
    "            Generated text response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.provider == \"anthropic\":\n",
    "                response = self.client.messages.create(\n",
    "                    model=self.model,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.content[0].text\n",
    "            \n",
    "            elif self.provider == \"openai\":\n",
    "                response = self.client.chat.completions.create(\n",
    "                    model=self.model,\n",
    "                    max_tokens=max_tokens,\n",
    "                    temperature=temperature,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "                )\n",
    "                return response.choices[0].message.content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"API call failed: {e}\")\n",
    "            return None\n",
    "            \n",
    "    def test_connection(self) -> bool:\n",
    "        \"\"\"\n",
    "        Test if the LLM API connection is working.\n",
    "        \"\"\"\n",
    "        test_prompt = \"Hello! Please respond with 'Connection successful' if you can read this.\"\n",
    "        response = self.call_llm(test_prompt, max_tokens=50)\n",
    "        \n",
    "        if response:\n",
    "            print(f\"{self.provider.title()} API connection successful\")\n",
    "            print(f\"Model: {self.model}\")\n",
    "            print(f\"Response: {response[:100]}...\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"{self.provider.title()} API connection failed\")\n",
    "            return False\n",
    "\n",
    "# Test API connections\n",
    "print(\"Testing API connections...\\n\")\n",
    "\n",
    "# Test Anthropic if available\n",
    "if anthropic_client:\n",
    "    try:\n",
    "        anthropic_llm = LLMInterface(provider=\"anthropic\")\n",
    "        anthropic_llm.test_connection()\n",
    "    except Exception as e:\n",
    "        print(f\"Anthropic connection error: {e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Test OpenAI if available  \n",
    "if openai_client:\n",
    "    try:\n",
    "        openai_llm = LLMInterface(provider=\"openai\")\n",
    "        openai_llm.test_connection()\n",
    "    except Exception as e:\n",
    "        print(f\"OpenAI connection error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa9270",
   "metadata": {},
   "source": [
    "## Method 1: Text-based Feature Selection (Li et al. 2024)\n",
    "\n",
    "This method uses dataset descriptions and feature metadata to guide LLM-based feature selection without requiring actual data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70e9f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-based Feature Selector class defined\n"
     ]
    }
   ],
   "source": [
    "class TextBasedFeatureSelector:\n",
    "    \"\"\"\n",
    "    Implements text-based feature selection using LLM semantic understanding.\n",
    "    Based on Li et al. 2024 - \"Exploring Large Language Models for Feature Selection\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_interface: LLMInterface):\n",
    "        self.llm = llm_interface\n",
    "        self.feature_scores = {}\n",
    "        \n",
    "    def create_dataset_description(self, dataset_info: Dict) -> str:\n",
    "        \"\"\"\n",
    "        Create a comprehensive dataset description for the LLM.\n",
    "        \"\"\"\n",
    "        name = dataset_info['name']\n",
    "        shape = dataset_info['shape']\n",
    "        columns = dataset_info['columns']\n",
    "        metadata = dataset_info.get('metadata', {})\n",
    "        \n",
    "        description = f\"\"\"\n",
    "Dataset: {name}\n",
    "Description: This is a tabular dataset with {shape[0]} samples and {shape[1]} features.\n",
    "Task: {metadata.get('task_type', 'Classification/Regression')}\n",
    "Target Variable: {columns[-1] if columns else 'Unknown'}\n",
    "Domain: {metadata.get('domain', 'General')}\n",
    "\n",
    "Features:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, col in enumerate(columns[:-1]):\n",
    "            description += f\"- {col}: {metadata.get('feature_descriptions', {}).get(col, 'Numerical/Categorical feature')}\\n\"\n",
    "            \n",
    "        return description.strip()\n",
    "    \n",
    "    def create_feature_selection_prompt(self, dataset_description: str, feature_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Create a prompt for feature importance scoring.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are an expert data scientist specializing in feature selection for machine learning.\n",
    "\n",
    "{dataset_description}\n",
    "\n",
    "Your task is to evaluate the importance of the feature \"{feature_name}\" for predicting the target variable.\n",
    "\n",
    "Please provide:\n",
    "1. An importance score between 0.0 and 1.0 (where 1.0 is most important)\n",
    "2. A brief reasoning for your score\n",
    "\n",
    "Format your response as JSON:\n",
    "{{\n",
    "    \"feature\": \"{feature_name}\",\n",
    "    \"importance_score\": 0.XX,\n",
    "    \"reasoning\": \"Brief explanation of why this feature is important/unimportant\"\n",
    "}}\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def score_feature(self, dataset_info: Dict, feature_name: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Score a single feature using LLM.\n",
    "        \"\"\"\n",
    "        dataset_desc = self.create_dataset_description(dataset_info)\n",
    "        prompt = self.create_feature_selection_prompt(dataset_desc, feature_name)\n",
    "        \n",
    "        response = self.llm.call_llm(prompt, max_tokens=300, temperature=0.1)\n",
    "        \n",
    "        if not response:\n",
    "            return {\"feature\": feature_name, \"importance_score\": 0.0, \"reasoning\": \"API call failed\"}\n",
    "        \n",
    "        try:\n",
    "            # Try to parse JSON response\n",
    "            result = json.loads(response)\n",
    "            return result\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: extract score from text\n",
    "            import re\n",
    "            score_match = re.search(r'\"importance_score\":\\s*([0-9.]+)', response)\n",
    "            score = float(score_match.group(1)) if score_match else 0.5\n",
    "            \n",
    "            return {\n",
    "                \"feature\": feature_name,\n",
    "                \"importance_score\": score,\n",
    "                \"reasoning\": \"Parsed from unstructured response\"\n",
    "            }\n",
    "    \n",
    "    def select_features(self, dataset_info: Dict, top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Select top features using text-based LLM approach.\n",
    "        \n",
    "        Args:\n",
    "            dataset_info: Dataset information dictionary\n",
    "            top_k: Number of top features to select (None for all)\n",
    "            \n",
    "        Returns:\n",
    "            List of feature scores sorted by importance\n",
    "        \"\"\"\n",
    "        features = dataset_info['columns'][:-1]  # Exclude target variable\n",
    "        results = []\n",
    "        \n",
    "        print(f\"Scoring {len(features)} features using text-based approach...\")\n",
    "        \n",
    "        for i, feature in enumerate(features):\n",
    "            print(f\"{i+1}/{len(features)}: {feature}\")\n",
    "            result = self.score_feature(dataset_info, feature)\n",
    "            results.append(result)\n",
    "            \n",
    "            # Small delay to be respectful to API\n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        # Sort by importance score\n",
    "        results.sort(key=lambda x: x['importance_score'], reverse=True)\n",
    "        \n",
    "        if top_k:\n",
    "            results = results[:top_k]\n",
    "            \n",
    "        self.feature_scores = {r['feature']: r['importance_score'] for r in results}\n",
    "        \n",
    "        return results\n",
    "\n",
    "print(\"Text-based Feature Selector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecefd4ee",
   "metadata": {},
   "source": [
    "## Method 2: LLM4FS Hybrid Approach (Li & Xiu 2025)\n",
    "\n",
    "This method combines LLM reasoning with traditional statistical methods by providing data samples to the LLM and instructing it to apply classical feature selection techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "879949c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM4FS Hybrid Selector class defined\n"
     ]
    }
   ],
   "source": [
    "class LLM4FS_HybridSelector:\n",
    "    \"\"\"\n",
    "    Implements LLM4FS hybrid feature selection approach.\n",
    "    Based on Li & Xiu 2025 - \"LLM4FS: Leveraging Large Language Models for Feature Selection\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, llm_interface: LLMInterface):\n",
    "        self.llm = llm_interface\n",
    "        self.methods = [\n",
    "            \"random_forest\", \"forward_selection\", \"backward_selection\",\n",
    "            \"recursive_feature_elimination\", \"mutual_information\", \"mrmr\"\n",
    "        ]\n",
    "        \n",
    "    def prepare_data_sample(self, df: pd.DataFrame, sample_size: int = 200) -> str:\n",
    "        \"\"\"\n",
    "        Prepare a data sample for the LLM (typically 200 samples as per paper).\n",
    "        \"\"\"\n",
    "        # Sample data (or use all if less than sample_size)\n",
    "        if len(df) > sample_size:\n",
    "            sample_df = df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            sample_df = df.copy()\n",
    "            \n",
    "        # Convert to CSV string format\n",
    "        return sample_df.to_csv(index=False)\n",
    "    \n",
    "    def create_hybrid_prompt(self, data_csv: str, task_type: str = \"classification\") -> str:\n",
    "        \"\"\"\n",
    "        Create prompt for hybrid LLM4FS approach.\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"\n",
    "You are an expert data scientist. Please analyze the following dataset and apply traditional feature selection methods to rank features by importance.\n",
    "\n",
    "Dataset (CSV format):\n",
    "{data_csv}\n",
    "\n",
    "Task: This is a {task_type} problem. The last column is the target variable.\n",
    "\n",
    "Please apply the following feature selection methods and provide importance scores:\n",
    "1. Random Forest feature importance\n",
    "2. Mutual Information\n",
    "3. Recursive Feature Elimination (RFE)\n",
    "4. Forward/Backward Selection\n",
    "\n",
    "For each feature (excluding the target), provide an importance score between 0.0 and 1.0.\n",
    "\n",
    "Format your response EXACTLY as JSON with this structure:\n",
    "{{\n",
    "    \"method\": \"hybrid_llm4fs\",\n",
    "    \"features\": [\n",
    "        {{\"name\": \"feature_name\", \"importance_score\": 0.XX, \"reasoning\": \"Brief explanation\"}},\n",
    "        {{\"name\": \"feature_name\", \"importance_score\": 0.XX, \"reasoning\": \"Brief explanation\"}}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Base your analysis on statistical relationships in the data, not just semantic understanding.\n",
    "\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def select_features(self, df: pd.DataFrame, dataset_info: Dict, top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Select features using LLM4FS hybrid approach.\n",
    "        \"\"\"\n",
    "        print(\"Applying LLM4FS hybrid feature selection...\")\n",
    "        \n",
    "        # Prepare data sample\n",
    "        data_csv = self.prepare_data_sample(df)\n",
    "        task_type = dataset_info.get('metadata', {}).get('task_type', 'classification')\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = self.create_hybrid_prompt(data_csv, task_type)\n",
    "        \n",
    "        # Make LLM call with higher token limit\n",
    "        response = self.llm.call_llm(prompt, max_tokens=3000, temperature=0.1)\n",
    "        \n",
    "        if not response:\n",
    "            print(\"API call failed\")\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Parse JSON response\n",
    "            result = json.loads(response)\n",
    "            features = result.get('features', [])\n",
    "            \n",
    "            # Sort by importance score\n",
    "            features.sort(key=lambda x: x.get('importance_score', 0), reverse=True)\n",
    "            \n",
    "            if top_k:\n",
    "                features = features[:top_k]\n",
    "                \n",
    "            return features\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse LLM response as complete JSON: {e}\")\n",
    "            print(\"Attempting to extract partial features from response...\")\n",
    "            \n",
    "            # Try to extract partial JSON from truncated response\n",
    "            features = self._extract_partial_features(response)\n",
    "            \n",
    "            if features:\n",
    "                print(f\"Successfully extracted {len(features)} features from partial response\")\n",
    "                features.sort(key=lambda x: x.get('importance_score', 0), reverse=True)\n",
    "                \n",
    "                if top_k:\n",
    "                    features = features[:top_k]\n",
    "                    \n",
    "                return features\n",
    "            else:\n",
    "                print(\"Could not extract features from response\")\n",
    "                print(f\"Response preview: {response[:300]}...\")\n",
    "                return []\n",
    "    \n",
    "    def _extract_partial_features(self, response: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract features from a potentially truncated JSON response.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Try to find feature objects even if JSON is incomplete\n",
    "        feature_pattern = r'\"name\":\\s*\"([^\"]+)\",\\s*\"importance_score\":\\s*([0-9.]+),\\s*\"reasoning\":\\s*\"([^\"]*)\"'\n",
    "        matches = re.findall(feature_pattern, response, re.DOTALL)\n",
    "        \n",
    "        for name, score, reasoning in matches:\n",
    "            try:\n",
    "                features.append({\n",
    "                    'name': name,\n",
    "                    'importance_score': float(score),\n",
    "                    'reasoning': reasoning[:100] + \"...\" if len(reasoning) > 100 else reasoning\n",
    "                })\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        return features\n",
    "            \n",
    "    \n",
    "    def _extract_partial_features(self, response: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Extract features from a potentially truncated JSON response.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        features = []\n",
    "        \n",
    "        # Try to find feature objects even if JSON is incomplete\n",
    "        feature_pattern = r'\"name\":\\s*\"([^\"]+)\",\\s*\"importance_score\":\\s*([0-9.]+),\\s*\"reasoning\":\\s*\"([^\"]*)\"'\n",
    "        matches = re.findall(feature_pattern, response)\n",
    "        \n",
    "        for name, score, reasoning in matches:\n",
    "            try:\n",
    "                features.append({\n",
    "                    'name': name,\n",
    "                    'importance_score': float(score),\n",
    "                    'reasoning': reasoning\n",
    "                })\n",
    "            except ValueError:\n",
    "                continue\n",
    "                \n",
    "        return features\n",
    "\n",
    "print(\"LLM4FS Hybrid Selector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62a7be1",
   "metadata": {},
   "source": [
    "## Method 3: Traditional Baseline Methods\n",
    "\n",
    "For comparison, we'll also implement traditional feature selection methods as baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7d96939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traditional Feature Selector class defined\n"
     ]
    }
   ],
   "source": [
    "class TraditionalFeatureSelector:\n",
    "    \"\"\"\n",
    "    Traditional feature selection methods for comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_scores = {}\n",
    "        \n",
    "    def mutual_information_selection(self, X: pd.DataFrame, y: pd.Series, top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Mutual information based feature selection.\n",
    "        \"\"\"\n",
    "        # Handle categorical target\n",
    "        if y.dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y)\n",
    "            scores = mutual_info_classif(X, y_encoded, random_state=42)\n",
    "        else:\n",
    "            from sklearn.feature_selection import mutual_info_regression\n",
    "            scores = mutual_info_regression(X, y, random_state=42)\n",
    "            \n",
    "        results = []\n",
    "        for i, feature in enumerate(X.columns):\n",
    "            results.append({\n",
    "                'name': feature,\n",
    "                'importance_score': scores[i],\n",
    "                'reasoning': 'Mutual information score'\n",
    "            })\n",
    "            \n",
    "        results.sort(key=lambda x: x['importance_score'], reverse=True)\n",
    "        \n",
    "        if top_k:\n",
    "            results = results[:top_k]\n",
    "            \n",
    "        return results\n",
    "        \n",
    "    def random_forest_selection(self, X: pd.DataFrame, y: pd.Series, top_k: int = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Random Forest based feature importance.\n",
    "        \"\"\"\n",
    "        # Handle categorical target\n",
    "        if y.dtype == 'object':\n",
    "            le = LabelEncoder()\n",
    "            y_encoded = le.fit_transform(y)\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        else:\n",
    "            y_encoded = y\n",
    "            rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            \n",
    "        rf.fit(X, y_encoded)\n",
    "        importances = rf.feature_importances_\n",
    "        \n",
    "        results = []\n",
    "        for i, feature in enumerate(X.columns):\n",
    "            results.append({\n",
    "                'name': feature,\n",
    "                'importance_score': importances[i],\n",
    "                'reasoning': 'Random Forest feature importance'\n",
    "            })\n",
    "            \n",
    "        results.sort(key=lambda x: x['importance_score'], reverse=True)\n",
    "        \n",
    "        if top_k:\n",
    "            results = results[:top_k]\n",
    "            \n",
    "        return results\n",
    "\n",
    "print(\"Traditional Feature Selector class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114cf68a",
   "metadata": {},
   "source": [
    "## Evaluation Framework\n",
    "\n",
    "Framework for comparing different feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f65c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Feature Selection Evaluator class defined\n"
     ]
    }
   ],
   "source": [
    "class FeatureSelectionEvaluator:\n",
    "    \"\"\"\n",
    "    Improved evaluator that properly detects classification vs regression tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        \n",
    "    def _is_classification_task(self, y: pd.Series) -> bool:\n",
    "        \"\"\"\n",
    "        Determine if this is a classification task based on target variable characteristics.\n",
    "        \"\"\"\n",
    "        # Check if dtype is object (string labels)\n",
    "        if y.dtype == 'object':\n",
    "            return True\n",
    "        \n",
    "        # Check if it's integer with small number of unique values (likely categorical)\n",
    "        if pd.api.types.is_integer_dtype(y):\n",
    "            n_unique = y.nunique()\n",
    "            n_samples = len(y)\n",
    "            # If less than 20 unique values OR less than 5% of samples are unique values\n",
    "            if n_unique <= 20 or (n_unique / n_samples) < 0.05:\n",
    "                return True\n",
    "        \n",
    "        # For continuous variables, assume regression\n",
    "        return False\n",
    "        \n",
    "    def evaluate_method(self, method_name: str, selected_features: List[str], \n",
    "                       X: pd.DataFrame, y: pd.Series) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate a feature selection method by training a model on selected features.\n",
    "        \"\"\"\n",
    "        if not selected_features:\n",
    "            return {'error': 'No features selected'}\n",
    "            \n",
    "        # Select features\n",
    "        X_selected = X[selected_features]\n",
    "        \n",
    "        # Determine task type\n",
    "        is_classification = self._is_classification_task(y)\n",
    "        \n",
    "        if is_classification:\n",
    "            # Classification task\n",
    "            if y.dtype == 'object':\n",
    "                le = LabelEncoder()\n",
    "                y_encoded = le.fit_transform(y)\n",
    "            else:\n",
    "                y_encoded = y  # Already numeric\n",
    "            model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            scoring = 'accuracy'\n",
    "        else:\n",
    "            # Regression task\n",
    "            y_encoded = y\n",
    "            model = LinearRegression()\n",
    "            scoring = 'r2'\n",
    "            \n",
    "        # Cross-validation\n",
    "        scores = cross_val_score(model, X_selected, y_encoded, cv=5, scoring=scoring)\n",
    "        \n",
    "        result = {\n",
    "            'method': method_name,\n",
    "            'num_features': len(selected_features),\n",
    "            'selected_features': selected_features,\n",
    "            'cv_scores': scores.tolist(),\n",
    "            'mean_score': scores.mean(),\n",
    "            'std_score': scores.std(),\n",
    "            'metric': scoring,\n",
    "            'task_type': 'classification' if is_classification else 'regression'\n",
    "        }\n",
    "        \n",
    "        self.results[method_name] = result\n",
    "        return result\n",
    "    \n",
    "    def compare_methods(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create comparison table of all evaluated methods.\n",
    "        \"\"\"\n",
    "        comparison_data = []\n",
    "        \n",
    "        for method_name, result in self.results.items():\n",
    "            if 'error' not in result:\n",
    "                comparison_data.append({\n",
    "                    'Method': method_name,\n",
    "                    'Num Features': result['num_features'],\n",
    "                    'Mean Score': result['mean_score'],\n",
    "                    'Std Score': result['std_score'],\n",
    "                    'Metric': result['metric']\n",
    "                })\n",
    "                \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        if not df.empty:\n",
    "            df = df.sort_values('Mean Score', ascending=False)\n",
    "            \n",
    "        return df\n",
    "    \n",
    "    def plot_comparison(self):\n",
    "        \"\"\"\n",
    "        Plot comparison of methods.\n",
    "        \"\"\"\n",
    "        df = self.compare_methods()\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No results to plot\")\n",
    "            return\n",
    "            \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Performance comparison\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(df['Method'], df['Mean Score'], yerr=df['Std Score'], capsize=5)\n",
    "        plt.title('Feature Selection Method Performance')\n",
    "        plt.ylabel(f'Score ({df[\"Metric\"].iloc[0]})')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        # Number of features\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(df['Method'], df['Num Features'])\n",
    "        plt.title('Number of Selected Features')\n",
    "        plt.ylabel('Number of Features')\n",
    "        plt.xticks(rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"Improved Feature Selection Evaluator class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0474e8",
   "metadata": {},
   "source": [
    "## Multi-Dataset Classification Experiment\n",
    "\n",
    "Comparing all methods on three classification datasets:\n",
    "1. **CMC** (Contraceptive Method Choice) - 3 classes, social/demographic data\n",
    "2. **Vehicle** - 4 classes, geometric/visual features \n",
    "3. **Electricity** - 2 classes, energy/time series data\n",
    "\n",
    "All datasets evaluated using **accuracy** as the performance metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33f31cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "MULTI-DATASET FEATURE SELECTION EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "==================== DATASET: CMC ====================\n",
      "Shape: (1473, 10)\n",
      "Features: 9\n",
      "Target: target\n",
      "Target distribution: {1: np.int64(629), 3: np.int64(511), 2: np.int64(333)}\n",
      "\n",
      "==================== DATASET: VEHICLE ====================\n",
      "Shape: (846, 19)\n",
      "Features: 18\n",
      "Target: target\n",
      "Target distribution: {'bus': np.int64(218), 'saab': np.int64(217), 'opel': np.int64(212)}\n",
      "\n",
      "==================== DATASET: ELECTRICITY ====================\n",
      "Shape: (45312, 9)\n",
      "Features: 8\n",
      "Target: target\n",
      "Target distribution: {'DOWN': np.int64(26075), 'UP': np.int64(19237)}\n",
      "\n",
      "Loaded 3 datasets successfully!\n"
     ]
    }
   ],
   "source": [
    "# Multi-dataset experiment setup\n",
    "datasets_to_test = [\"cmc\", \"vehicle\", \"electricity\"]\n",
    "all_results = {}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-DATASET FEATURE SELECTION EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name in datasets_to_test:\n",
    "    print(f\"\\n{'='*20} DATASET: {dataset_name.upper()} {'='*20}\")\n",
    "    \n",
    "    # Load dataset\n",
    "    df, metadata = load_dataset(dataset_name)\n",
    "    dataset_info = get_dataset_info(dataset_name)\n",
    "    \n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Features: {len(dataset_info['columns'][:-1])}\")\n",
    "    print(f\"Target: {dataset_info['columns'][-1]}\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    \n",
    "    print(f\"Target distribution: {dict(y.value_counts().head(3))}\")\n",
    "    \n",
    "    # Store dataset info\n",
    "    all_results[dataset_name] = {\n",
    "        'dataset_info': dataset_info,\n",
    "        'X': X,\n",
    "        'y': y,\n",
    "        'df': df\n",
    "    }\n",
    "\n",
    "print(f\"\\nLoaded {len(datasets_to_test)} datasets successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb137a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All methods initialized for multi-dataset experiment\n"
     ]
    }
   ],
   "source": [
    "# Initialize methods and results storage\n",
    "llm = LLMInterface(provider=\"anthropic\")\n",
    "text_selector = TextBasedFeatureSelector(llm)\n",
    "hybrid_selector = LLM4FS_HybridSelector(llm)\n",
    "traditional_selector = TraditionalFeatureSelector()\n",
    "\n",
    "# Storage for all experiment results\n",
    "experiment_results = {}\n",
    "performance_summary = []\n",
    "\n",
    "print(\"All methods initialized for multi-dataset experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3788e4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING DATASET: CMC\n",
      "============================================================\n",
      "\n",
      "1. TEXT-BASED FEATURE SELECTION\n",
      "----------------------------------------\n",
      "Scoring 9 features using text-based approach...\n",
      "1/9: Wifes_age\n",
      "2/9: Wifes_education\n",
      "3/9: Husbands_education\n",
      "4/9: Number_of_children_ever_born\n",
      "5/9: Wifes_religion\n",
      "6/9: Wifes_now_working%3F\n",
      "7/9: Husbands_occupation\n",
      "8/9: Standard-of-living_index\n",
      "9/9: Media_exposure\n",
      "Selected: ['Wifes_age', 'Wifes_education', 'Standard-of-living_index', 'Husbands_education', 'Number_of_children_ever_born']\n",
      "\n",
      "2. LLM4FS HYBRID APPROACH\n",
      "----------------------------------------\n",
      "Applying LLM4FS hybrid feature selection...\n",
      "Selected: ['Standard-of-living_index', 'Number_of_children_ever_born', 'Wifes_education', 'Wifes_age', 'Husbands_education']\n",
      "\n",
      "3. TRADITIONAL METHODS\n",
      "----------------------------------------\n",
      "Random Forest selected: ['Wifes_age', 'Number_of_children_ever_born', 'Wifes_education', 'Standard-of-living_index', 'Husbands_occupation']\n",
      "Mutual Information selected: ['Number_of_children_ever_born', 'Husbands_education', 'Wifes_education', 'Husbands_occupation', 'Wifes_age']\n",
      "\n",
      "CMC PERFORMANCE:\n",
      "            Method  Num Features  Mean Score  Std Score   Metric\n",
      "Mutual Information             5    0.527488   0.024588 accuracy\n",
      "        Text-based             5    0.519343   0.019274 accuracy\n",
      "     LLM4FS Hybrid             5    0.519343   0.019274 accuracy\n",
      "     Random Forest             5    0.510508   0.018876 accuracy\n",
      "\n",
      "============================================================\n",
      "PROCESSING DATASET: VEHICLE\n",
      "============================================================\n",
      "\n",
      "1. TEXT-BASED FEATURE SELECTION\n",
      "----------------------------------------\n",
      "Scoring 18 features using text-based approach...\n",
      "1/18: COMPACTNESS\n",
      "2/18: CIRCULARITY\n",
      "3/18: DISTANCE_CIRCULARITY\n",
      "4/18: RADIUS_RATIO\n",
      "5/18: PR.AXIS_ASPECT_RATIO\n",
      "6/18: MAX.LENGTH_ASPECT_RATIO\n",
      "7/18: SCATTER_RATIO\n",
      "8/18: ELONGATEDNESS\n",
      "9/18: PR.AXIS_RECTANGULARITY\n",
      "10/18: MAX.LENGTH_RECTANGULARITY\n",
      "11/18: SCALED_VARIANCE_MAJOR\n",
      "12/18: SCALED_VARIANCE_MINOR\n",
      "13/18: SCALED_RADIUS_OF_GYRATION\n",
      "14/18: SKEWNESS_ABOUT_MAJOR\n",
      "15/18: SKEWNESS_ABOUT_MINOR\n",
      "16/18: KURTOSIS_ABOUT_MAJOR\n",
      "17/18: KURTOSIS_ABOUT_MINOR\n",
      "18/18: HOLLOWS_RATIO\n",
      "Selected: ['COMPACTNESS', 'CIRCULARITY', 'DISTANCE_CIRCULARITY', 'RADIUS_RATIO', 'PR.AXIS_ASPECT_RATIO']\n",
      "\n",
      "2. LLM4FS HYBRID APPROACH\n",
      "----------------------------------------\n",
      "Applying LLM4FS hybrid feature selection...\n",
      "Selected: ['COMPACTNESS', 'SCALED_VARIANCE_MAJOR', 'RADIUS_RATIO', 'SCATTER_RATIO', 'CIRCULARITY']\n",
      "\n",
      "3. TRADITIONAL METHODS\n",
      "----------------------------------------\n",
      "Random Forest selected: ['MAX.LENGTH_ASPECT_RATIO', 'SCALED_VARIANCE_MINOR', 'MAX.LENGTH_RECTANGULARITY', 'DISTANCE_CIRCULARITY', 'SCALED_VARIANCE_MAJOR']\n",
      "Mutual Information selected: ['SCALED_VARIANCE_MINOR', 'SCATTER_RATIO', 'ELONGATEDNESS', 'PR.AXIS_RECTANGULARITY', 'SCALED_VARIANCE_MAJOR']\n",
      "\n",
      "VEHICLE PERFORMANCE:\n",
      "            Method  Num Features  Mean Score  Std Score   Metric\n",
      "     Random Forest             5    0.635969   0.037523 accuracy\n",
      "        Text-based             5    0.586251   0.020143 accuracy\n",
      "     LLM4FS Hybrid             5    0.543773   0.025564 accuracy\n",
      "Mutual Information             5    0.404309   0.037105 accuracy\n",
      "\n",
      "============================================================\n",
      "PROCESSING DATASET: ELECTRICITY\n",
      "============================================================\n",
      "\n",
      "1. TEXT-BASED FEATURE SELECTION\n",
      "----------------------------------------\n",
      "Scoring 8 features using text-based approach...\n",
      "1/8: date\n",
      "2/8: day\n",
      "3/8: period\n",
      "4/8: nswprice\n",
      "5/8: nswdemand\n",
      "6/8: vicprice\n",
      "7/8: vicdemand\n",
      "8/8: transfer\n",
      "Selected: ['period', 'nswprice', 'nswdemand', 'date', 'day']\n",
      "\n",
      "2. LLM4FS HYBRID APPROACH\n",
      "----------------------------------------\n",
      "Applying LLM4FS hybrid feature selection...\n",
      "Selected: ['period', 'vicprice', 'nswprice', 'transfer', 'nswdemand']\n",
      "\n",
      "3. TRADITIONAL METHODS\n",
      "----------------------------------------\n",
      "Random Forest selected: ['nswprice', 'date', 'nswdemand', 'vicprice', 'period']\n",
      "Mutual Information selected: ['nswprice', 'vicprice', 'date', 'nswdemand', 'period']\n",
      "\n",
      "ELECTRICITY PERFORMANCE:\n",
      "            Method  Num Features  Mean Score  Std Score   Metric\n",
      "     LLM4FS Hybrid             5    0.742009   0.060941 accuracy\n",
      "     Random Forest             5    0.710648   0.063948 accuracy\n",
      "Mutual Information             5    0.710648   0.063948 accuracy\n",
      "        Text-based             5    0.705837   0.056937 accuracy\n",
      "\n",
      "============================================================\n",
      "ALL DATASETS PROCESSED\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Run experiments on all datasets\n",
    "for dataset_name in datasets_to_test:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING DATASET: {dataset_name.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Get dataset components\n",
    "    dataset_info = all_results[dataset_name]['dataset_info']\n",
    "    X = all_results[dataset_name]['X']\n",
    "    y = all_results[dataset_name]['y']\n",
    "    df = all_results[dataset_name]['df']\n",
    "    \n",
    "    # Initialize evaluator for this dataset\n",
    "    evaluator = FeatureSelectionEvaluator()\n",
    "    dataset_results = {}\n",
    "    \n",
    "    print(f\"\\n1. TEXT-BASED FEATURE SELECTION\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        text_results = text_selector.select_features(dataset_info, top_k=5)\n",
    "        text_features = [r['feature'] for r in text_results]\n",
    "        \n",
    "        if text_features:\n",
    "            print(f\"Selected: {text_features}\")\n",
    "            text_eval = evaluator.evaluate_method(\"Text-based\", text_features, X, y)\n",
    "            dataset_results['text'] = {\n",
    "                'features': text_features,\n",
    "                'performance': text_eval\n",
    "            }\n",
    "        else:\n",
    "            print(\"Text-based method failed\")\n",
    "            dataset_results['text'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Text-based method error: {e}\")\n",
    "        dataset_results['text'] = None\n",
    "    \n",
    "    print(f\"\\n2. LLM4FS HYBRID APPROACH\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        hybrid_results = hybrid_selector.select_features(df, dataset_info, top_k=5)\n",
    "        hybrid_features = [r['name'] for r in hybrid_results] if hybrid_results else []\n",
    "        \n",
    "        if hybrid_features:\n",
    "            print(f\"Selected: {hybrid_features}\")\n",
    "            hybrid_eval = evaluator.evaluate_method(\"LLM4FS Hybrid\", hybrid_features, X, y)\n",
    "            dataset_results['hybrid'] = {\n",
    "                'features': hybrid_features,\n",
    "                'performance': hybrid_eval\n",
    "            }\n",
    "        else:\n",
    "            print(\"LLM4FS Hybrid method failed\")\n",
    "            dataset_results['hybrid'] = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"LLM4FS Hybrid method error: {e}\")\n",
    "        dataset_results['hybrid'] = None\n",
    "    \n",
    "    print(f\"\\n3. TRADITIONAL METHODS\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        # Random Forest\n",
    "        rf_results = traditional_selector.random_forest_selection(X, y, top_k=5)\n",
    "        rf_features = [r['name'] for r in rf_results]\n",
    "        print(f\"Random Forest selected: {rf_features}\")\n",
    "        rf_eval = evaluator.evaluate_method(\"Random Forest\", rf_features, X, y)\n",
    "        \n",
    "        # Mutual Information\n",
    "        mi_results = traditional_selector.mutual_information_selection(X, y, top_k=5)\n",
    "        mi_features = [r['name'] for r in mi_results]\n",
    "        print(f\"Mutual Information selected: {mi_features}\")\n",
    "        mi_eval = evaluator.evaluate_method(\"Mutual Information\", mi_features, X, y)\n",
    "        \n",
    "        dataset_results['random_forest'] = {\n",
    "            'features': rf_features,\n",
    "            'performance': rf_eval\n",
    "        }\n",
    "        dataset_results['mutual_info'] = {\n",
    "            'features': mi_features,\n",
    "            'performance': mi_eval\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Traditional methods error: {e}\")\n",
    "        dataset_results['random_forest'] = None\n",
    "        dataset_results['mutual_info'] = None\n",
    "    \n",
    "    # Store results for this dataset\n",
    "    experiment_results[dataset_name] = {\n",
    "        'evaluator': evaluator,\n",
    "        'results': dataset_results\n",
    "    }\n",
    "    \n",
    "    # Show performance for this dataset\n",
    "    comparison_df = evaluator.compare_methods()\n",
    "    if not comparison_df.empty:\n",
    "        print(f\"\\n{dataset_name.upper()} PERFORMANCE:\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "        \n",
    "        # Add to summary\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            performance_summary.append({\n",
    "                'Dataset': dataset_name,\n",
    "                'Method': row['Method'],\n",
    "                'Score': row['Mean Score'],\n",
    "                'Std': row['Std Score'],\n",
    "                'Metric': row['Metric']\n",
    "            })\n",
    "    else:\n",
    "        print(f\"\\nNo valid results for {dataset_name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ALL DATASETS PROCESSED\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47c1e3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SELECTED FEATURES BY METHOD AND DATASET\n",
      "============================================================\n",
      "\n",
      "CMC DATASET:\n",
      "------------------------------\n",
      "Text              : ['Wifes_age', 'Wifes_education', 'Standard-of-living_index', 'Husbands_education', 'Number_of_children_ever_born']\n",
      "Hybrid            : ['Standard-of-living_index', 'Number_of_children_ever_born', 'Wifes_education', 'Wifes_age', 'Husbands_education']\n",
      "Random Forest     : ['Wifes_age', 'Number_of_children_ever_born', 'Wifes_education', 'Standard-of-living_index', 'Husbands_occupation']\n",
      "Mutual Info       : ['Number_of_children_ever_born', 'Husbands_education', 'Wifes_education', 'Husbands_occupation', 'Wifes_age']\n",
      "Consensus (2+)    : ['Wifes_education', 'Husbands_education', 'Number_of_children_ever_born', 'Wifes_age', 'Standard-of-living_index', 'Husbands_occupation']\n",
      "\n",
      "VEHICLE DATASET:\n",
      "------------------------------\n",
      "Text              : ['COMPACTNESS', 'CIRCULARITY', 'DISTANCE_CIRCULARITY', 'RADIUS_RATIO', 'PR.AXIS_ASPECT_RATIO']\n",
      "Hybrid            : ['COMPACTNESS', 'SCALED_VARIANCE_MAJOR', 'RADIUS_RATIO', 'SCATTER_RATIO', 'CIRCULARITY']\n",
      "Random Forest     : ['MAX.LENGTH_ASPECT_RATIO', 'SCALED_VARIANCE_MINOR', 'MAX.LENGTH_RECTANGULARITY', 'DISTANCE_CIRCULARITY', 'SCALED_VARIANCE_MAJOR']\n",
      "Mutual Info       : ['SCALED_VARIANCE_MINOR', 'SCATTER_RATIO', 'ELONGATEDNESS', 'PR.AXIS_RECTANGULARITY', 'SCALED_VARIANCE_MAJOR']\n",
      "Consensus (2+)    : ['SCATTER_RATIO', 'DISTANCE_CIRCULARITY', 'CIRCULARITY', 'SCALED_VARIANCE_MINOR', 'RADIUS_RATIO', 'SCALED_VARIANCE_MAJOR', 'COMPACTNESS']\n",
      "\n",
      "ELECTRICITY DATASET:\n",
      "------------------------------\n",
      "Text              : ['period', 'nswprice', 'nswdemand', 'date', 'day']\n",
      "Hybrid            : ['period', 'vicprice', 'nswprice', 'transfer', 'nswdemand']\n",
      "Random Forest     : ['nswprice', 'date', 'nswdemand', 'vicprice', 'period']\n",
      "Mutual Info       : ['nswprice', 'vicprice', 'date', 'nswdemand', 'period']\n",
      "Consensus (2+)    : ['nswdemand', 'vicprice', 'date', 'period', 'nswprice']\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection Patterns Analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SELECTED FEATURES BY METHOD AND DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for dataset_name in datasets_to_test:\n",
    "    print(f\"\\n{dataset_name.upper()} DATASET:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    if dataset_name in experiment_results:\n",
    "        results = experiment_results[dataset_name]['results']\n",
    "        \n",
    "        for method_name, method_data in results.items():\n",
    "            if method_data and 'features' in method_data:\n",
    "                features = method_data['features']\n",
    "                method_display = method_name.replace('_', ' ').title()\n",
    "                print(f\"{method_display:18}: {features}\")\n",
    "        \n",
    "        # Find consensus features\n",
    "        all_features = set()\n",
    "        method_features = {}\n",
    "        \n",
    "        for method_name, method_data in results.items():\n",
    "            if method_data and 'features' in method_data:\n",
    "                features = method_data['features']\n",
    "                method_features[method_name] = set(features)\n",
    "                all_features.update(features)\n",
    "        \n",
    "        if len(method_features) > 1:\n",
    "            feature_counts = {}\n",
    "            for feature in all_features:\n",
    "                count = sum(1 for features in method_features.values() if feature in features)\n",
    "                feature_counts[feature] = count\n",
    "            \n",
    "            consensus_features = [f for f, count in feature_counts.items() if count >= 2]\n",
    "            if consensus_features:\n",
    "                print(f\"{'Consensus (2+)':18}: {consensus_features}\")\n",
    "    else:\n",
    "        print(\"No results available\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
